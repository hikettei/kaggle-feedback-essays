{"cells":[{"source":"<a href=\"https://www.kaggle.com/hikettei/feedbackprize?scriptVersionId=89268759\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"fa3cd53f","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:13:19.456359Z","iopub.status.busy":"2022-03-04T11:13:19.454862Z","iopub.status.idle":"2022-03-04T11:13:29.605338Z","shell.execute_reply":"2022-03-04T11:13:29.605785Z","shell.execute_reply.started":"2022-03-04T10:57:32.392655Z"},"papermill":{"duration":10.172292,"end_time":"2022-03-04T11:13:29.606044","exception":false,"start_time":"2022-03-04T11:13:19.433752","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2.6.2\n","Found GPU at: /device:GPU:0\n"]},{"name":"stderr","output_type":"stream","text":["2022-03-04 11:13:25.145943: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-03-04 11:13:25.198268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:13:25.199218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:13:25.199893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:13:29.587978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:13:29.588760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:13:29.589389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:13:29.589974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","import transformers\n","import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import glob\n","from tqdm import tqdm\n","import math\n","import timeit\n","\n","device_name = tf.test.gpu_device_name()\n","if \"GPU\" not in device_name:\n","    print(\"GPU device not found\")\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":2,"id":"78dd153f","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:13:29.63888Z","iopub.status.busy":"2022-03-04T11:13:29.638226Z","iopub.status.idle":"2022-03-04T11:13:29.640943Z","shell.execute_reply":"2022-03-04T11:13:29.640507Z","shell.execute_reply.started":"2022-03-04T10:58:22.193724Z"},"papermill":{"duration":0.020197,"end_time":"2022-03-04T11:13:29.641054","exception":false,"start_time":"2022-03-04T11:13:29.620857","status":"completed"},"tags":[]},"outputs":[],"source":["max_length = 227"]},{"cell_type":"code","execution_count":3,"id":"5d75651e","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:13:29.68015Z","iopub.status.busy":"2022-03-04T11:13:29.679251Z","iopub.status.idle":"2022-03-04T11:14:28.361436Z","shell.execute_reply":"2022-03-04T11:14:28.360957Z","shell.execute_reply.started":"2022-03-04T10:58:23.347407Z"},"papermill":{"duration":58.706606,"end_time":"2022-03-04T11:14:28.361602","exception":false,"start_time":"2022-03-04T11:13:29.654996","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 15594/15594 [00:55<00:00, 280.13it/s]\n"]}],"source":["train = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"train.csv\"))\n","train[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n","\n","train[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\n","train[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n","\n","train_txt = glob.glob('../input/feedback-prize-2021/train/*.txt') \n","\n","cols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\n","train[cols_to_display].head()\n","\n","# this code chunk is copied from Rob Mulla\n","len_dict = {}\n","word_dict = {}\n","for t in tqdm(train_txt):\n","    with open(t, \"r\") as txt_file:\n","        myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n","        data = txt_file.read()\n","        mylen = len(data.strip())\n","        myword = len(data.split())\n","        len_dict[myid] = mylen\n","        word_dict[myid] = myword\n","train[\"essay_len\"] = train[\"id\"].map(len_dict)\n","train[\"essay_words\"] = train[\"id\"].map(word_dict)\n","\n","data_ids = train['id'].unique()"]},{"cell_type":"code","execution_count":4,"id":"1d424acd","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:14:28.72401Z","iopub.status.busy":"2022-03-04T11:14:28.722949Z","iopub.status.idle":"2022-03-04T11:14:57.6978Z","shell.execute_reply":"2022-03-04T11:14:57.697245Z","shell.execute_reply.started":"2022-03-04T10:59:29.391566Z"},"papermill":{"duration":29.155358,"end_time":"2022-03-04T11:14:57.697932","exception":false,"start_time":"2022-03-04T11:14:28.542574","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 144292/144292 [00:28<00:00, 5008.39it/s]\n","/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>discourse_start</th>\n","      <th>discourse_end</th>\n","      <th>discourse_type</th>\n","      <th>essay_len</th>\n","      <th>gap_length</th>\n","      <th>gap_end_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>144270</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>0</td>\n","      <td>317</td>\n","      <td>Lead</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144271</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>318</td>\n","      <td>515</td>\n","      <td>Position</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144272</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>684</td>\n","      <td>692</td>\n","      <td>Claim</td>\n","      <td>3140</td>\n","      <td>167.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144273</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>693</td>\n","      <td>710</td>\n","      <td>Claim</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144274</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>714</td>\n","      <td>724</td>\n","      <td>Claim</td>\n","      <td>3140</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144275</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>725</td>\n","      <td>1360</td>\n","      <td>Evidence</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144276</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>1361</td>\n","      <td>1471</td>\n","      <td>Claim</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144277</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>1472</td>\n","      <td>1881</td>\n","      <td>Evidence</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144278</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>1882</td>\n","      <td>2019</td>\n","      <td>Claim</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144279</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>2029</td>\n","      <td>2123</td>\n","      <td>Claim</td>\n","      <td>3140</td>\n","      <td>8.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144280</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>2123</td>\n","      <td>2702</td>\n","      <td>Evidence</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144281</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>2703</td>\n","      <td>2799</td>\n","      <td>Counterclaim</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144282</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>2817</td>\n","      <td>2907</td>\n","      <td>Rebuttal</td>\n","      <td>3140</td>\n","      <td>16.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>144283</th>\n","      <td>AFEC37C2D43F</td>\n","      <td>2907</td>\n","      <td>3140</td>\n","      <td>Concluding Statement</td>\n","      <td>3140</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  id  discourse_start  discourse_end        discourse_type  \\\n","144270  AFEC37C2D43F                0            317                  Lead   \n","144271  AFEC37C2D43F              318            515              Position   \n","144272  AFEC37C2D43F              684            692                 Claim   \n","144273  AFEC37C2D43F              693            710                 Claim   \n","144274  AFEC37C2D43F              714            724                 Claim   \n","144275  AFEC37C2D43F              725           1360              Evidence   \n","144276  AFEC37C2D43F             1361           1471                 Claim   \n","144277  AFEC37C2D43F             1472           1881              Evidence   \n","144278  AFEC37C2D43F             1882           2019                 Claim   \n","144279  AFEC37C2D43F             2029           2123                 Claim   \n","144280  AFEC37C2D43F             2123           2702              Evidence   \n","144281  AFEC37C2D43F             2703           2799          Counterclaim   \n","144282  AFEC37C2D43F             2817           2907              Rebuttal   \n","144283  AFEC37C2D43F             2907           3140  Concluding Statement   \n","\n","        essay_len  gap_length  gap_end_length  \n","144270       3140         NaN             NaN  \n","144271       3140         NaN             NaN  \n","144272       3140       167.0             NaN  \n","144273       3140         NaN             NaN  \n","144274       3140         2.0             NaN  \n","144275       3140         NaN             NaN  \n","144276       3140         NaN             NaN  \n","144277       3140         NaN             NaN  \n","144278       3140         NaN             NaN  \n","144279       3140         8.0             NaN  \n","144280       3140         NaN             NaN  \n","144281       3140         NaN             NaN  \n","144282       3140        16.0             NaN  \n","144283       3140         NaN             NaN  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#initialize column\n","train['gap_length'] = np.nan\n","\n","#set the first one\n","train.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n","\n","#loop over rest\n","for i in tqdm(range(1, len(train))):\n","    #gap if difference is not 1 within an essay\n","    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n","        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n","        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n","        #minus 2 as the previous end is always -1 and the previous start always +1\n","    #gap if the first discourse of an new essay does not start at 0\n","    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n","        and (train.loc[i, \"discourse_start\"] != 0)):\n","        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n","\n","\n"," #is there any text after the last discourse of an essay?\n","last_ones = train.drop_duplicates(subset=\"id\", keep='last')\n","last_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n","                                       (last_ones.essay_len - last_ones.discourse_end),\\\n","                                       np.nan)\n","\n","cols_to_merge = ['id', 'discourse_id', 'gap_end_length']\n","train = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")\n","\n","#display an example\n","cols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\n","train[cols_to_display].query('id == \"AFEC37C2D43F\"')"]},{"cell_type":"code","execution_count":5,"id":"44868772","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:14:58.22861Z","iopub.status.busy":"2022-03-04T11:14:58.226977Z","iopub.status.idle":"2022-03-04T11:14:58.229216Z","shell.execute_reply":"2022-03-04T11:14:58.229613Z","shell.execute_reply.started":"2022-03-04T11:00:33.005214Z"},"papermill":{"duration":0.273899,"end_time":"2022-03-04T11:14:58.22979","exception":false,"start_time":"2022-03-04T11:14:57.955891","status":"completed"},"tags":[]},"outputs":[],"source":["def prepare_text_data(file_name):\n","    with open(f\"../input/feedback-prize-2021/train/{file_name}.txt\") as f:\n","        txt = f.read()\n","    return [nltk.pos_tag(nltk.word_tokenize(parag)) for parag in re.split(\"\\n\\n\", txt)]\n","\n","def add_gap_rows(essay):\n","    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n","    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n","\n","    #index new row\n","    insert_row = len(df_essay)\n","   \n","    for i in range(1, len(df_essay)):          \n","        if df_essay.loc[i,\"gap_length\"] >0:\n","            if i == 0:\n","                start = 0 #as there is no i-1 for first row\n","                end = df_essay.loc[0, 'discourse_start'] -1\n","                disc_type = \"Nothing\"\n","                gap_end = np.nan\n","                gap = np.nan\n","                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n","                insert_row += 1\n","            else:\n","                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n","                end = df_essay.loc[i, 'discourse_start'] -1\n","                disc_type = \"Nothing\"\n","                gap_end = np.nan\n","                gap = np.nan\n","                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n","                insert_row += 1\n","    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n","\n","    #add gap at end\n","    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n","        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n","        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n","        disc_type = \"Nothing\"\n","        gap_end = np.nan\n","        gap = np.nan\n","        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n","        \n","    return(df_essay)\n","\n","def prepare_train_data(essay):\n","    df_essay = add_gap_rows(essay)\n","    #code from https://www.kaggle.com/odins0n/feedback-prize-eda, but adjusted to df_essay\n","    essay_file = \"../input/feedback-prize-2021/train/\" + essay + \".txt\"\n","    items = []\n","    p = 0\n","    with open(essay_file, 'r') as file: data = file.read()\n","    \n","    for i, row in df_essay.iterrows():\n","        p = int(row['discourse_start'])\n","        e = int(row['discourse_end'])\n","        items.append([data[p:e], row['discourse_type']])\n","\n","    return items"]},{"cell_type":"code","execution_count":6,"id":"045e1606","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:14:58.759822Z","iopub.status.busy":"2022-03-04T11:14:58.759069Z","iopub.status.idle":"2022-03-04T11:14:59.670526Z","shell.execute_reply":"2022-03-04T11:14:59.671226Z","shell.execute_reply.started":"2022-03-04T11:00:23.627038Z"},"papermill":{"duration":1.17404,"end_time":"2022-03-04T11:14:59.671443","exception":false,"start_time":"2022-03-04T11:14:58.497403","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","#Initialize tokenizer\n","model_name = \"../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased\"\n","bert_tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n","\n","dtype_to_num = {\"Lead\":0,\n","                \"Position\":1,\n","                \"Claim\":2,\n","                \"Counterclaim\":3,\n","                \"Rebuttal\":4,\n","                \"Evidence\":5,\n","                \"Concluding Statement\":6,\n","                \"Nothing\":7}\n","\n","num_to_dtype = {v: k for k, v in dtype_to_num.items()}\n","\n","num_class_type = 8"]},{"cell_type":"code","execution_count":7,"id":"2b0e5b91","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:00.323669Z","iopub.status.busy":"2022-03-04T11:15:00.322579Z","iopub.status.idle":"2022-03-04T11:15:00.324698Z","shell.execute_reply":"2022-03-04T11:15:00.325155Z","shell.execute_reply.started":"2022-03-04T11:00:25.54932Z"},"papermill":{"duration":0.275145,"end_time":"2022-03-04T11:15:00.32531","exception":false,"start_time":"2022-03-04T11:15:00.050165","status":"completed"},"tags":[]},"outputs":[],"source":["def collect_train_sets():\n","    train_lines = []\n","    train_labels = []\n","\n","    max_data_size = 10000\n","    max_length    = 50\n","    max_sentence_size = 0\n","\n","\n","    for i in tqdm(range(max_data_size)):\n","        for txt in prepare_train_data(data_ids[i]):\n","            l = len(bert_tokenizer.tokenize(txt[0]))\n","            if l <= max_length:\n","                train_lines.append(txt[0])\n","                train_labels.append(dtype_to_num[txt[1]])\n","            else:\n","                for t in re.split(\"[.\\n]\", txt[0]):\n","                    tsize = len(bert_tokenizer.tokenize(t))\n","                    if tsize <= 100:\n","                        max_sentence_size = max([max_sentence_size, tsize])\n","                        train_lines.append(t)\n","                        train_labels.append(dtype_to_num[txt[1]])\n","                    else:\n","                        for t1 in re.split(\"[,]\", t):\n","                            max_sentence_size = max([max_sentence_size, len(bert_tokenizer.tokenize(t1))])\n","                            train_lines.append(t1)\n","                            train_labels.append(dtype_to_num[txt[1]])\n","\n","    max_length = max_sentence_size\n","    return [train_lines, train_labels, max_length]"]},{"cell_type":"code","execution_count":8,"id":"ea7997da","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:00.867864Z","iopub.status.busy":"2022-03-04T11:15:00.867288Z","iopub.status.idle":"2022-03-04T11:15:00.870954Z","shell.execute_reply":"2022-03-04T11:15:00.870516Z","shell.execute_reply.started":"2022-03-04T10:51:05.972029Z"},"papermill":{"duration":0.280477,"end_time":"2022-03-04T11:15:00.871087","exception":false,"start_time":"2022-03-04T11:15:00.59061","status":"completed"},"tags":[]},"outputs":[],"source":["data_ids = []\n","train = []\n","df_essay = []\n","last_ones = []\n","len_dict = {}\n","word_dict = {}\n","train_txt = []\n","data = 0"]},{"cell_type":"code","execution_count":9,"id":"10e50c08","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:01.398149Z","iopub.status.busy":"2022-03-04T11:15:01.397159Z","iopub.status.idle":"2022-03-04T11:15:01.40042Z","shell.execute_reply":"2022-03-04T11:15:01.400862Z","shell.execute_reply.started":"2022-03-04T11:00:39.153746Z"},"papermill":{"duration":0.276834,"end_time":"2022-03-04T11:15:01.401009","exception":false,"start_time":"2022-03-04T11:15:01.124175","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["           name  size\n","0             _  2574\n","1            _4  2574\n","2           _i5  2335\n","3          tqdm  2008\n","4           _i4  1567\n","..          ...   ...\n","65            i    28\n","66         data    24\n","67  __package__    16\n","68     __spec__    16\n","69   __loader__    16\n","\n","[70 rows x 2 columns]\n"]}],"source":["import sys\n","import pandas as pd\n","\n","print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n","                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True))"]},{"cell_type":"code","execution_count":10,"id":"3e6fff99","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:01.932041Z","iopub.status.busy":"2022-03-04T11:15:01.931174Z","iopub.status.idle":"2022-03-04T11:15:01.937473Z","shell.execute_reply":"2022-03-04T11:15:01.937065Z","shell.execute_reply.started":"2022-03-04T11:00:41.61165Z"},"papermill":{"duration":0.283425,"end_time":"2022-03-04T11:15:01.937607","exception":false,"start_time":"2022-03-04T11:15:01.654182","status":"completed"},"tags":[]},"outputs":[],"source":["def make_train_data(datas):\n","    shape = (len(datas), max_length)\n","    \n","    input_ids = np.zeros(shape, dtype=\"int32\")\n","    attention_mask = np.zeros(shape, dtype=\"int32\")\n","    token_type_ids = np.zeros(shape, dtype=\"int32\")\n","    \n","    for i, data in enumerate(datas):\n","        encoded = bert_tokenizer.encode_plus(datas[i],\n","                                             max_length=max_length,\n","                                             pad_to_max_length=True,\n","                                             truncation=True)\n","        input_ids[i] = encoded[\"input_ids\"]\n","        attention_mask[i] = encoded[\"attention_mask\"]\n","        token_type_ids[i] = encoded[\"token_type_ids\"] \n","    \n","    return [input_ids, attention_mask, token_type_ids]\n","\n","def build_model():\n","    input_shape = (max_length,)\n","    \n","    input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n","    attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n","    token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n","    \n","    bert_model = transformers.TFBertModel.from_pretrained(model_name)\n","    \n","    bert_output = bert_model(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids\n","    )\n","    last_hidden_state = bert_output.last_hidden_state\n","    pooler_output     = bert_output.pooler_output\n","    \n","    output = tf.keras.layers.Dense(num_class_type, activation=\"softmax\")(pooler_output)\n","    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n","    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n","    return model"]},{"cell_type":"code","execution_count":11,"id":"e66cc57d","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:02.458933Z","iopub.status.busy":"2022-03-04T11:15:02.45837Z","iopub.status.idle":"2022-03-04T11:15:02.462069Z","shell.execute_reply":"2022-03-04T11:15:02.461631Z","shell.execute_reply.started":"2022-03-04T11:00:45.255601Z"},"papermill":{"duration":0.267947,"end_time":"2022-03-04T11:15:02.462192","exception":false,"start_time":"2022-03-04T11:15:02.194245","status":"completed"},"tags":[]},"outputs":[],"source":["def train():\n","    train_lines, train_labels, max_length = collect_train_sets()\n","    batch_size = 256\n","    epoch = 15\n","\n","    test_size = 15\n","    train_line_size = len(train_lines)\n","\n","    X_train = make_train_data(train_lines[test_size:train_line_size])\n","    Y_train = tf.keras.utils.to_categorical(train_labels[test_size:train_line_size], num_classes=num_class_type)\n","\n","    X_test  = make_train_data(train_lines[0:test_size])\n","    Y_test  = tf.keras.utils.to_categorical(train_labels[0:test_size], num_classes=num_class_type)\n","    \n","    model = build_model()\n","    model.summary()\n","    \n","    model.fit(\n","        X_train,\n","        Y_train,\n","        batch_size=batch_size,\n","        epochs=epoch)\n","    return model"]},{"cell_type":"code","execution_count":12,"id":"84894111","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:02.976003Z","iopub.status.busy":"2022-03-04T11:15:02.975029Z","iopub.status.idle":"2022-03-04T11:15:21.313827Z","shell.execute_reply":"2022-03-04T11:15:21.314267Z","shell.execute_reply.started":"2022-03-04T11:11:44.627702Z"},"papermill":{"duration":18.596356,"end_time":"2022-03-04T11:15:21.314434","exception":false,"start_time":"2022-03-04T11:15:02.718078","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-03-04 11:15:03.837623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.838441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.839038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.839955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.840581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.841178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.841836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.842428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-03-04 11:15:03.842992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","Some layers from the model checkpoint at ../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at ../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["model = build_model()\n","model.load_weights(\"../input/berttrainedmodel1/test_model2.h5\")"]},{"cell_type":"code","execution_count":13,"id":"7b3fb08a","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:22.097989Z","iopub.status.busy":"2022-03-04T11:15:22.097313Z","iopub.status.idle":"2022-03-04T11:15:30.349413Z","shell.execute_reply":"2022-03-04T11:15:30.348504Z","shell.execute_reply.started":"2022-03-04T11:12:25.227376Z"},"papermill":{"duration":8.657817,"end_time":"2022-03-04T11:15:30.34956","exception":false,"start_time":"2022-03-04T11:15:21.691743","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","2022-03-04 11:15:22.208144: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"]}],"source":["test_txt = glob.glob('../input/feedback-prize-2021/test/*.txt') \n","sub = []\n","\n","for t in test_txt:\n","    with open(t, \"r\") as txt_file:\n","        myid          = t.split(\"/\")[-1].replace(\".txt\", \"\")\n","        datas         = re.split(\"[.,\\n]\", txt_file.read())\n","        input_X       = make_train_data(datas)\n","        with tf.device('/gpu:0'):\n","            X_predict     = model.predict(input_X)\n","        X_predict_num = np.argmax(X_predict, axis=1)\n","        p = 0\n","        for i, data in enumerate(datas):\n","            if len(data) == 0:\n","                p += 1\n","            else:\n","                word_count = len(bert_tokenizer.tokenize(data))\n","                if X_predict_num[i] == 7:\n","                    pass\n","                else:\n","                    word_list = ' '.join([str(x) for x in range(p, p+word_count)])\n","                    sub.append((myid, num_to_dtype[X_predict_num[i]], word_list))\n","                p += word_count\n","    \n","df = pd.DataFrame(sub)\n","df.columns = ['id','class','predictionstring']\n","df.to_csv('submission.csv',index=False)"]},{"cell_type":"code","execution_count":14,"id":"e152e89d","metadata":{"execution":{"iopub.execute_input":"2022-03-04T11:15:30.877551Z","iopub.status.busy":"2022-03-04T11:15:30.876611Z","iopub.status.idle":"2022-03-04T11:15:30.88094Z","shell.execute_reply":"2022-03-04T11:15:30.880471Z","shell.execute_reply.started":"2022-03-04T11:12:39.342504Z"},"papermill":{"duration":0.272754,"end_time":"2022-03-04T11:15:30.881076","exception":false,"start_time":"2022-03-04T11:15:30.608322","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>class</th>\n","      <th>predictionstring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Evidence</td>\n","      <td>0 1 2 3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Lead</td>\n","      <td>4 5 6 7 8 9 10 11 12 13 14 15 16 17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Evidence</td>\n","      <td>18 19 20 21 22 23 24 25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Lead</td>\n","      <td>26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 4...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0FB0700DAF44</td>\n","      <td>Lead</td>\n","      <td>70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 8...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>283</th>\n","      <td>D46BCB48440A</td>\n","      <td>Concluding Statement</td>\n","      <td>371 372 373 374 375 376 377 378 379 380</td>\n","    </tr>\n","    <tr>\n","      <th>284</th>\n","      <td>D46BCB48440A</td>\n","      <td>Concluding Statement</td>\n","      <td>381 382 383 384 385 386 387</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>D46BCB48440A</td>\n","      <td>Concluding Statement</td>\n","      <td>388 389 390 391 392 393 394 395</td>\n","    </tr>\n","    <tr>\n","      <th>286</th>\n","      <td>D46BCB48440A</td>\n","      <td>Concluding Statement</td>\n","      <td>396 397 398 399 400 401 402 403 404</td>\n","    </tr>\n","    <tr>\n","      <th>287</th>\n","      <td>D46BCB48440A</td>\n","      <td>Evidence</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>288 rows × 3 columns</p>\n","</div>"],"text/plain":["               id                 class  \\\n","0    0FB0700DAF44              Evidence   \n","1    0FB0700DAF44                  Lead   \n","2    0FB0700DAF44              Evidence   \n","3    0FB0700DAF44                  Lead   \n","4    0FB0700DAF44                  Lead   \n","..            ...                   ...   \n","283  D46BCB48440A  Concluding Statement   \n","284  D46BCB48440A  Concluding Statement   \n","285  D46BCB48440A  Concluding Statement   \n","286  D46BCB48440A  Concluding Statement   \n","287  D46BCB48440A              Evidence   \n","\n","                                      predictionstring  \n","0                                              0 1 2 3  \n","1                  4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n","2                              18 19 20 21 22 23 24 25  \n","3    26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 4...  \n","4    70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 8...  \n","..                                                 ...  \n","283            371 372 373 374 375 376 377 378 379 380  \n","284                        381 382 383 384 385 386 387  \n","285                    388 389 390 391 392 393 394 395  \n","286                396 397 398 399 400 401 402 403 404  \n","287                                                     \n","\n","[288 rows x 3 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":143.480551,"end_time":"2022-03-04T11:15:34.342919","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-03-04T11:13:10.862368","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}