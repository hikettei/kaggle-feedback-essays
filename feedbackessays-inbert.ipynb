{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport transformers\n!pip install -q pyyaml h5py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T03:11:46.705673Z","iopub.execute_input":"2022-03-04T03:11:46.705902Z","iopub.status.idle":"2022-03-04T03:11:55.092371Z","shell.execute_reply.started":"2022-03-04T03:11:46.705880Z","shell.execute_reply":"2022-03-04T03:11:55.091123Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport re\nimport torch\nimport glob\nfrom tqdm import tqdm\nimport math\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:10:05.377607Z","iopub.execute_input":"2022-03-04T02:10:05.377858Z","iopub.status.idle":"2022-03-04T02:10:06.577332Z","shell.execute_reply.started":"2022-03-04T02:10:05.377830Z","shell.execute_reply":"2022-03-04T02:10:06.576560Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"nltk.download(\"all\")","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:10:10.905971Z","iopub.execute_input":"2022-03-04T02:10:10.906366Z","iopub.status.idle":"2022-03-04T02:10:29.513427Z","shell.execute_reply.started":"2022-03-04T02:10:10.906337Z","shell.execute_reply":"2022-03-04T02:10:29.512535Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"train.csv\"))\ntrain[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\ntrain_txt = glob.glob('../input/feedback-prize-2021/train/*.txt') \n\ncols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\ntrain[cols_to_display].head()\n\n# this code chunk is copied from Rob Mulla\nlen_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)\n\ndata_ids = train['id'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:10:29.516467Z","iopub.execute_input":"2022-03-04T02:10:29.517148Z","iopub.status.idle":"2022-03-04T02:11:44.689098Z","shell.execute_reply.started":"2022-03-04T02:10:29.517115Z","shell.execute_reply":"2022-03-04T02:11:44.688338Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#initialize column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n        #minus 2 as the previous end is always -1 and the previous start always +1\n    #gap if the first discourse of an new essay does not start at 0\n    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n\n\n #is there any text after the last discourse of an essay?\nlast_ones = train.drop_duplicates(subset=\"id\", keep='last')\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id', 'discourse_id', 'gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")\n\n#display an example\ncols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\ntrain[cols_to_display].query('id == \"AFEC37C2D43F\"')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:13:41.723336Z","iopub.execute_input":"2022-03-04T02:13:41.724699Z","iopub.status.idle":"2022-03-04T02:14:05.927811Z","shell.execute_reply.started":"2022-03-04T02:13:41.724643Z","shell.execute_reply":"2022-03-04T02:14:05.926846Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def prepare_text_data(file_name):\n    with open(f\"../input/feedback-prize-2021/train/{file_name}.txt\") as f:\n        txt = f.read()\n    return [nltk.pos_tag(nltk.word_tokenize(parag)) for parag in re.split(\"\\n\\n\", txt)]\n\ndef add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)\n\ndef prepare_train_data(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https://www.kaggle.com/odins0n/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"../input/feedback-prize-2021/train/\" + essay + \".txt\"\n    items = []\n    p = 0\n    with open(essay_file, 'r') as file: data = file.read()\n    \n    for i, row in df_essay.iterrows():\n        p = int(row['discourse_start'])\n        e = int(row['discourse_end'])\n        items.append([data[p:e], row['discourse_type']])\n\n    return items\n\n#prepare_train_data(data_ids[8])","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:14:10.622791Z","iopub.execute_input":"2022-03-04T02:14:10.623064Z","iopub.status.idle":"2022-03-04T02:14:10.642517Z","shell.execute_reply.started":"2022-03-04T02:14:10.623036Z","shell.execute_reply":"2022-03-04T02:14:10.641738Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#Initialize tokenizer\nmodel_name = \"../input/huggingface-bert-variants/bert-base-cased/bert-base-cased\"\nbert_tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n\ndtype_to_num = {\"Lead\":0,\n                \"Position\":1,\n                \"Claim\":2,\n                \"Counterclaim\":3,\n                \"Rebuttal\":4,\n                \"Evidence\":5,\n                \"Concluding Statement\":6,\n                \"Nothing\":7}\n\nnum_class_type = 8\n\ntpu_use = False\n\nif tpu_use:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T03:09:09.619224Z","iopub.execute_input":"2022-03-04T03:09:09.619453Z","iopub.status.idle":"2022-03-04T03:09:09.659919Z","shell.execute_reply.started":"2022-03-04T03:09:09.619431Z","shell.execute_reply":"2022-03-04T03:09:09.658892Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"train_lines = []\ntrain_labels = []\n\nmax_data_size = 100\nmax_length    = 0\n\n\nfor i in tqdm(range(max_data_size)):\n    for txt in prepare_train_data(data_ids[i]):\n        max_length = max([max_length, len(bert_tokenizer.tokenize(txt[0]))])\n        train_lines.append(txt[0])\n        train_labels.append(dtype_to_num[txt[1]])","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:47:01.811994Z","iopub.execute_input":"2022-03-04T02:47:01.812254Z","iopub.status.idle":"2022-03-04T02:47:04.048141Z","shell.execute_reply.started":"2022-03-04T02:47:01.812226Z","shell.execute_reply":"2022-03-04T02:47:04.047383Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def make_train_data(datas):\n    shape = (len(datas), max_length)\n    \n    input_ids = np.zeros(shape, dtype=\"int32\")\n    attention_mask = np.zeros(shape, dtype=\"int32\")\n    token_type_ids = np.zeros(shape, dtype=\"int32\")\n    \n    for i, data in enumerate(datas):\n        encoded = bert_tokenizer.encode_plus(datas[i],\n                                             max_length=max_length,\n                                             pad_to_max_length=True,\n                                             truncation=True)\n        input_ids[i] = encoded[\"input_ids\"]\n        attention_mask[i] = encoded[\"attention_mask\"]\n        token_type_ids[i] = encoded[\"token_type_ids\"] \n    \n    return [input_ids, attention_mask, token_type_ids]\n\ndef build_model():\n    input_shape = (max_length,)\n    \n    input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n    attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n    token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n    \n    bert_model = transformers.TFBertModel.from_pretrained(model_name)\n    \n    bert_output = bert_model(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids\n    )\n    last_hidden_state = bert_output.last_hidden_state\n    pooler_output     = bert_output.pooler_output\n    \n    output = tf.keras.layers.Dense(num_class_type, activation=\"softmax\")(pooler_output)\n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-04T03:05:40.204617Z","iopub.execute_input":"2022-03-04T03:05:40.204833Z","iopub.status.idle":"2022-03-04T03:05:40.214946Z","shell.execute_reply.started":"2022-03-04T03:05:40.204811Z","shell.execute_reply":"2022-03-04T03:05:40.213837Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"batch_size = 15\nepoch = 10\n\ntest_size = 5\ntrain_line_size = len(train_lines)\n\nX_train = make_train_data(train_lines[test_size:train_line_size])\nY_train = tf.keras.utils.to_categorical(train_labels[test_size:train_line_size], num_classes=num_class_type)\n\nX_test  = make_train_data(train_lines[0:test_size])\nY_test  = tf.keras.utils.to_categorical(train_labels[0:test_size], num_classes=num_class_type)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T03:05:47.084109Z","iopub.execute_input":"2022-03-04T03:05:47.084446Z","iopub.status.idle":"2022-03-04T03:05:48.115641Z","shell.execute_reply.started":"2022-03-04T03:05:47.084422Z","shell.execute_reply":"2022-03-04T03:05:48.114829Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"model = build_model()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T03:05:50.664713Z","iopub.execute_input":"2022-03-04T03:05:50.664949Z","iopub.status.idle":"2022-03-04T03:05:58.604296Z","shell.execute_reply.started":"2022-03-04T03:05:50.664917Z","shell.execute_reply":"2022-03-04T03:05:58.603422Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"model.fit(\n    X_train,\n    Y_train,\n    batch_size=batch_size,\n    epochs=epoch\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T03:06:12.140185Z","iopub.execute_input":"2022-03-04T03:06:12.140785Z","iopub.status.idle":"2022-03-04T03:07:38.257429Z","shell.execute_reply.started":"2022-03-04T03:06:12.140751Z","shell.execute_reply":"2022-03-04T03:07:38.256259Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"ytest = np.asarray(Y_test)\n\npredict_result = model.predict(X_test)\ny_pred = np.argmax(predict_result, axis=1)\n\nprint([ytest, y_pred])","metadata":{"execution":{"iopub.status.busy":"2022-03-04T03:04:41.936914Z","iopub.execute_input":"2022-03-04T03:04:41.937362Z","iopub.status.idle":"2022-03-04T03:04:44.887378Z","shell.execute_reply.started":"2022-03-04T03:04:41.937335Z","shell.execute_reply":"2022-03-04T03:04:44.886366Z"},"trusted":true},"execution_count":83,"outputs":[]}]}